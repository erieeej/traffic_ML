{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## UK Road Safety: Traffic Accidents and Vehicles\n",
    " #### (Introduction, Data Cleaning, and Feature Manipulation)\n",
    " \n",
    "#Problem: Traffic Accidents<br>\n",
    "#Solution Method: Use data to figure out how to lower the number of accidents and the severity of them.\n",
    "### Table of Contents\n",
    "[Importing and Data Merging](#Importing-and-Data-Merging)<br>\n",
    "[Data Cleaning](#Data-Cleaning)<br>\n",
    "[Nulls and Outliers](#Nulls-and-Outliers)<br>\n",
    "[Feature Manipulation Creation and Engineering](#Feature-Manipulation-Creation-and-Engineering)<br>\n",
    "### Importing and Data Merging\n",
    "#Import modules\n",
    "import numpy as np\n",
    "import holidays\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "#scipy\n",
    "import scipy.stats as stats\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "#sklearn\n",
    "import sklearn\n",
    "from sklearn import ensemble\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score \n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "#other learners\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "#imblearn\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "#webscraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "#time series\n",
    "import statsmodels.api as sm\n",
    "from pylab import rcParams\n",
    "import itertools\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "\n",
    "#warning ignorer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# #DATAFRAME PICKLE (with cleaning etc.)CREATED IN CELLS BELOW INSTEAD OF RUNNING THROUGH ENTIRE PROCESS AFTER RESTARTING\n",
    "# #import pickled file\n",
    "# df = pd.read_pickle(\"df.pkl\")\n",
    "#import files\n",
    "\n",
    "ac = pd.read_csv(r'Accident_Information.csv', low_memory=False, chunksize=30000)\n",
    "vc = pd.read_csv(r'Vehicle_Information.csv', low_memory=False, chunksize=30000)\n",
    "Previously, I did not remove \"Data missing or out of range\" from the datasets however through cleaning and checking the value counts I decided to do so for sanity purposes only. Most of the percentages that had this as a value were not a high percentage either. \n",
    "#chunk cleaning and dataframing for accident column\n",
    "acchunk = []\n",
    "for chunk in ac:\n",
    "    acchunk_filter = chunk[\n",
    "        (chunk.Year.astype(int) >= 2010) &\n",
    "        (chunk.Year.astype(int) <= 2017) &\n",
    "        (chunk['Road_Type'] != \"Unknown\") &\n",
    "        (chunk['Junction_Control'] != \"Data missing or out of range\") &\n",
    "        (chunk['Carriageway_Hazards'] != \"Data missing or out of range\") &\n",
    "        (chunk['Junction_Detail'] != \"Data missing or out of range\") &\n",
    "        (chunk['Road_Surface_Conditions'] != \"Data missing or out of range\") &\n",
    "        (chunk['Special_Conditions_at_Site'] != \"Data missing or out of range\") &\n",
    "        (chunk['Weather_Conditions'] != \"Data missing or out of range\") &\n",
    "        (chunk['Latitude'].notnull()) &\n",
    "        (chunk['Longitude'].notnull())\n",
    "    ]\n",
    "    acchunk.append(acchunk_filter)\n",
    "df1 = pd.concat(acchunk)\n",
    "\n",
    "#chunk cleaning for vehicles column\n",
    "vcchunk = []\n",
    "for chunk2 in vc:\n",
    "    vcchunk_filter = chunk2[\n",
    "        (chunk2.Year.astype(int) >= 2010)&\n",
    "        (chunk2.Year.astype(int) <= 2017) &\n",
    "        (chunk2['Driver_Home_Area_Type'] != \"Data missing or out of range\") &\n",
    "        (chunk2['Journey_Purpose_of_Driver'] != \"Data missing or out of range\") &\n",
    "        (chunk2['Junction_Location'] != \"Data missing or out of range\") &\n",
    "        (chunk2['Was_Vehicle_Left_Hand_Drive'] != \"Data missing or out of range\") &\n",
    "        (chunk2['Hit_Object_in_Carriageway'] != \"Data missing or out of range\") &\n",
    "        (chunk2['Skidding_and_Overturning'] != \"Data missing or out of range\") &\n",
    "        (chunk2['Towing_and_Articulation'] != \"Data missing or out of range\") &\n",
    "        (chunk2['Vehicle_Leaving_Carriageway'] != \"Data missing or out of range\") &\n",
    "        (chunk2['Vehicle_Manoeuvre'] != \"Data missing or out of range\") &\n",
    "        (chunk2['Vehicle_Type'] != \"Data missing or out of range\") &\n",
    "        (chunk2['X1st_Point_of_Impact'] != \"Data missing or out of range\") &\n",
    "        (chunk2['Sex_of_Driver'] != \"Data missing or out of range\") &\n",
    "        (chunk2['Age_Band_of_Driver'] != \"Data missing or out of range\")\n",
    "        \n",
    "    ]\n",
    "    vcchunk.append(vcchunk_filter)\n",
    "df2 = pd.concat(vcchunk)\n",
    "#check columns\n",
    "print(\"Accident's Columns:\\n\",df1.columns, \"\\n\")\n",
    "\n",
    "print(\"Vehicle's Columns:\\n\",df2.columns)\n",
    "print('Accident Shape', df1.shape)\n",
    "print('Vehicle Shape',df2.shape)\n",
    "#merge dataframes\n",
    "df = pd.merge(df1,df2)\n",
    "#check columns\n",
    "print(\"Names of Combined Columns:\\n\",df.columns, \"\\n\")\n",
    "print(\"\\nShape:\\n\",df.shape)\n",
    "df.describe(include ='all')\n",
    "### Data Cleaning\n",
    "#check corr b/t Location_Easting_OSGR & Location_Northing_OSGR AND Longitude and Latitude\n",
    "\n",
    "print(df['Location_Easting_OSGR'].corr(df['Longitude']))\n",
    "\n",
    "\n",
    "print(df['Location_Northing_OSGR'].corr(df['Latitude']))\n",
    "#drop Location_Easting_OSGR & Location_Northing_OSGR\n",
    "#because they are the similar to Latitude and Longitude \n",
    "\n",
    "df = df.drop(['Location_Easting_OSGR', 'Location_Northing_OSGR'], axis=1)\n",
    "df.shape\n",
    "#standardize all column names to lowercase, and remove some characters \n",
    "#for ease of use in querying\n",
    "df.columns = map(str.lower, df.columns)\n",
    "df.columns = df.columns.str.replace('.','')\n",
    "df.columns = df.columns.str.replace('(','')\n",
    "df.columns = df.columns.str.replace(')','')\n",
    "#convert date/time to datetime datatype\n",
    "\n",
    "df['date'] = pd.to_datetime((df['date']), format= \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "#df.dtypes\n",
    "#mistyped datatypes\n",
    "\n",
    "df[['did_police_officer_attend_scene_of_accident',\n",
    "    'driver_imd_decile','vehicle_reference',\n",
    "    'vehicle_locationrestricted_lane','1st_road_number',\n",
    "    '2nd_road_number','driver_imd_decile',\n",
    "    'pedestrian_crossing-physical_facilities',\n",
    "   'pedestrian_crossing-human_control']]= df[['did_police_officer_attend_scene_of_accident',\n",
    "    'driver_imd_decile','vehicle_reference',\n",
    "    'vehicle_locationrestricted_lane','1st_road_number',\n",
    "    '2nd_road_number','driver_imd_decile',\n",
    "    'pedestrian_crossing-physical_facilities',\n",
    "   'pedestrian_crossing-human_control']].astype('object')\n",
    "\n",
    "\n",
    "df.columns.to_series().groupby(df.dtypes).groups\n",
    "#### Nulls and Outliers\n",
    "df.isnull().sum().sort_values(ascending=False)/df.shape[0]*100\n",
    "##### 2nd_road_class\n",
    "# #2nd_road_class\n",
    "df['2nd_road_class'].value_counts()/df.shape[0]*100\n",
    "With 40% of non null being unclassified and 39% of the overall 2nd_road_class column being null, I have decided to drop it in it's entirely. \n",
    "df = df.drop(['2nd_road_class'], axis=1)\n",
    "##### driver_imd_decile\n",
    "#driver_imd_decile\n",
    "df['driver_imd_decile'].value_counts()/df.shape[0]*100\n",
    "Since the distribution of categories for 'driver_imd_decile seem very similar, I've decided not to use the mode but \"method='ffill'\"\n",
    "df['driver_imd_decile'].fillna(method='ffill', inplace=True)\n",
    "##### age_of_vehicle\n",
    "df['age_of_vehicle'].describe()\n",
    "df['age_of_vehicle'].median()\n",
    "Changing the nulls of \"age of vehicle\" to median, then creating it as a category\n",
    "#fillna by 7 \n",
    "df['age_of_vehicle'].fillna(7, inplace=True)\n",
    "\n",
    "#group age_of_vehicle\n",
    "#1=0-3, 2=3-5, 3=5-8, 4=8-11, 5=\n",
    "def fixedvehicleage(age):\n",
    "    if age>=0 and age<=120:\n",
    "        return age\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df['age_of_vehicle'] = df['age_of_vehicle'].apply(fixedvehicleage)\n",
    "\n",
    "\n",
    "df['age_of_vehicle'] = pd.cut(df['age_of_vehicle'], \n",
    "                             [0,2,5,8,11,14,17,120], labels=['1', '2', '3','4','5','6','7'])\n",
    "\n",
    "##### Model\n",
    "#model\n",
    "df['model'].value_counts()/df.shape[0]*100\n",
    "df['model'].describe()\n",
    "Knowing that there are 28824 unique models for the model column I have decided to use the ffill method on it as well. \n",
    "df['model'].fillna(method='ffill', inplace=True)\n",
    "Note: A lot of the values of \"model' are labeled as \"missing\". I do not want to change these because the model could have actually been missing from the car from the accident or it could not be recognizable at the time of the accident.\n",
    "#### engine_capacity_cc\n",
    "#engine_capacity_cc\n",
    "df['engine_capacity_cc'].describe()\n",
    "I am going to handle both outliers and the null values of engine_capacity_cc using the ideals of quantiles and the interquartile range (IQR).\n",
    "#first I'm going to handle both ends of outliers.\n",
    "#(determine the min and max cuttoffs for detecting the outlier)\n",
    "q75, q25 = np.percentile(df['engine_capacity_cc'].dropna(), [75 ,25])\n",
    "iqr = q75 - q25\n",
    " \n",
    "ecmin = q25 - (iqr*1.5)\n",
    "ecmax = q75 + (iqr*1.5)\n",
    "\n",
    "print(ecmax)\n",
    "print(ecmin)\n",
    "To explain, what I am going to do is use the ecmax number for the maximum engine_capacity_cc and ecmin for my engine_capacity_cc. Then I'm going to take the mean of those and use it as my fillna.\n",
    "df = df[df['engine_capacity_cc']<=ecmax]\n",
    "df = df[df['engine_capacity_cc']>=ecmin]\n",
    "df['engine_capacity_cc'].hist(bins=20)\n",
    "plt.style.use('dark_background')\n",
    "I can accept this distribution and will now check and handle their nulls\n",
    "#check values of 'engine_capacity_cc'\n",
    "df['engine_capacity_cc'].describe()\n",
    "df['engine_capacity_cc'].mean()\n",
    "Going to round this mean value\n",
    "df['engine_capacity_cc'].fillna(1652, inplace=True)\n",
    "Note: After doing the above null fixes, propulsion_code dropped from having 10% null values to 0. (see below). I will continue on and fix lsoa_of_accident_location then drop the rest of the null values with are all <5%.\n",
    "df.isnull().sum().sort_values(ascending=False)/df.shape[0]*100\n",
    "#### lsoa_of_accident_location\n",
    "# #lsoa_of_accident_location\n",
    "df['lsoa_of_accident_location'].value_counts()\n",
    "df['lsoa_of_accident_location'].describe()\n",
    "With 35061 unique variable and a high count amount the top variables I am deciding to do ffill again. \n",
    "df['lsoa_of_accident_location'].fillna(method='ffill', inplace=True)\n",
    "#### Check nulls for again\n",
    "df.isnull().sum().sort_values(ascending=False)/df.shape[0]*100\n",
    "Dropping the remaining nulls that are <1%.\n",
    "#drop the remaining nulls that are <1%\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#last check\n",
    "df.isnull().sum().sort_values(ascending=False)/df.shape[0]*100\n",
    "df.shape\n",
    "df.info()\n",
    "#### More outliers, categorizing, and other cleanup\n",
    "#detecting outliers of numerical columns (all floats/ints excluding lat/long and year)\n",
    "\n",
    "df_num = df[['engine_capacity_cc','number_of_casualties','number_of_vehicles','speed_limit']]\n",
    "\n",
    "df_num.hist( bins=25, grid=False, figsize=(12,8))\n",
    "plt.style.use('dark_background')\n",
    "Column 'speed_limit' seems ok and was previously altered 'engine_capacity_cc'. However, 'number_of_casualties', and 'number_of_vehicles',will be evaluated. \n",
    "# #number_of_casualties\n",
    "df['number_of_casualties'].value_counts()\n",
    "#create casualities grouping\n",
    "\n",
    "def casualities(num_cas):\n",
    "    if num_cas >=1 and num_cas <2:\n",
    "        return \"1\"\n",
    "    elif num_cas >=2 and num_cas <3:\n",
    "        return \"2\"\n",
    "    elif num_cas >=3 and num_cas <4:\n",
    "        return \"3\"\n",
    "    elif num_cas >= 4 and num_cas <5:\n",
    "        return \"4\"\n",
    "    elif num_cas >= 5:\n",
    "        return \"5+\"\n",
    "  \n",
    "    \n",
    "#apply function   \n",
    "df['number_of_casualties']= df['number_of_casualties'].apply(casualities)\n",
    "#number_of_casualties\n",
    "df['number_of_casualties'].value_counts()\n",
    "df['propulsion_code'].value_counts()/df.shape[0]*100\n",
    "#Clean the values for Propulsion Code. \n",
    "df['propulsion_code'] = df['propulsion_code'].replace(to_replace=\"Gas\", value=\"Petrol\")\n",
    "df['propulsion_code'] = df['propulsion_code'].replace(to_replace=\"Gas/Bi-fuel\", value=\"Bio-fuel\")\n",
    "df['propulsion_code'] = df['propulsion_code'].replace(to_replace=\"Petrol/Gas (LPG)\", value=\"LPG Petrol\")\n",
    "df['propulsion_code'] = df['propulsion_code'].replace(to_replace=\"Gas Diesel\", value=\"Diesel\")\n",
    "df['propulsion_code'].value_counts()/df.shape[0]*100\n",
    "### Feature Manipulation Creation and Engineering\n",
    "\n",
    "# #unique values\n",
    "df.nunique().sort_values(ascending=False)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['month'] = df ['date'].apply(lambda time: time.month)\n",
    "#creating a weekend feature that includes Friday-Sunday\n",
    "df['weekend']= np.where(df['day_of_week'].isin(['Friday', 'Saturday', 'Sunday']), 1, 0)\n",
    "#create time of day feature with Morning Rush, Day, Noon Rush, Afternoon, After Work Rush, Night\n",
    "\n",
    "#time of day dictionary\n",
    "timeofdaygroups = {1: \"Morning Rush (6-10)\",\n",
    "                   2: \"Day (10-12)\",\n",
    "                   3: \"Lunch Rush (12-14)\",\n",
    "                   4: \"Afternoon (14-16)\",\n",
    "                   5: \"After Work Rush (16-18)\",\n",
    "                   6: \"Evening (18-22)\",\n",
    "                   7: \"Night (22-6)\"}\n",
    "#pull time data and create hour column\n",
    "df['hour'] = df['time'].str[0:2]\n",
    " \n",
    "#convert to numeric    \n",
    "df['hour'] =  pd.to_numeric(df['hour'])\n",
    "\n",
    "#convert to integer\n",
    "df['hour'] = df['hour'].astype('int')\n",
    "#create time_of_day grouping\n",
    "\n",
    "def daygroup(hour):\n",
    "    if hour >= 6 and hour < 10:\n",
    "        return \"1\"\n",
    "    elif hour >= 10 and hour < 12:\n",
    "        return \"2\"\n",
    "    elif hour >= 12 and hour < 14:\n",
    "        return \"3\"\n",
    "    elif hour >= 14 and hour < 16:\n",
    "        return \"4\"\n",
    "    elif hour >= 16 and hour < 18:\n",
    "        return \"5\"\n",
    "    elif hour >= 18 and hour < 22:\n",
    "        return \"6\"\n",
    "    else:\n",
    "        return \"7\"\n",
    "    \n",
    "#apply function   \n",
    "#time of day function\n",
    "df['time_of_day']= df['hour'].apply(daygroup)   \n",
    "df[['weekend','day_of_week','time', 'time_of_day']].tail(10)\n",
    "#vehicle_type\n",
    "df['vehicle_type'].value_counts()/df.shape[0]*100\n",
    "I want to condense the vehicle type variables. \n",
    "#motorcycles\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Motorcycle over 500cc\", \n",
    "                                                        value=\"Motorcycle\")\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\n",
    "                                                        \"Motorcycle over 125cc and up to 500cc\",\n",
    "                                                        value=\"Motorcycle\")\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Motorcycle 125cc and under\", \n",
    "                                                value=\"Motorcycle\")\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Motorcycle 50cc and under\", \n",
    "                                                        value=\"Motorcycle\")\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Electric motorcycle\", \n",
    "                                                        value=\"Motorcycle\")\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Motorcycle - unknown cc\", \n",
    "                                                        value=\"Motorcycle\")\n",
    "\n",
    "\n",
    "#Goods_vehicle\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\n",
    "                                                        \"Van / Goods 3.5 tonnes mgw or under\", \n",
    "                                                        value=\"Goods Vehicle\")\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Goods over 3.5t. and under 7.5t\", \n",
    "                                                        value=\"Goods Vehicle\")\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Goods vehicle - unknown weight\", \n",
    "                                                        value=\"Goods Vehicle\")\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Goods 7.5 tonnes mgw and over\", \n",
    "                                                        value=\"Goods Vehicle\")\n",
    "\n",
    "#car\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Taxi/Private hire car\", \n",
    "                                                        value=\"Car\")\n",
    "\n",
    "\n",
    "#bus\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Minibus (8 - 16 passenger seats)\", \n",
    "                                                        value=\"Bus\")\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\n",
    "                                                        \"Bus or coach (17 or more pass seats)\", \n",
    "                                                        value=\"Bus\")\n",
    "\n",
    "#other vehicle\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Agricultural vehicle\", \n",
    "                                                        value=\"Other Vehicle\")\n",
    "df['vehicle_type'] = df['vehicle_type'].replace(to_replace=\"Other vehicle\", \n",
    "                                                        value=\"Other Vehicle\")\n",
    "\n",
    "#vehicle_type\n",
    "df['vehicle_type'].value_counts()/df.shape[0]*100\n",
    "Create more condense groups for age band of driver in order to deal with some potential outliers.\n",
    "#age_band_of_driver \n",
    "df['age_band_of_driver'].value_counts()/df.shape[0]*100\n",
    "#I did this before hand because as \"Over 75\", it wouldnt convert in the codes below\n",
    "df['age_band_of_driver']=df['age_band_of_driver'].replace(\"Over 75\",\"75-100\")\n",
    "age1 = [\"0 - 5\", \"6 - 10\", \"11 - 15\"]\n",
    "age2 = [\"16 - 20\",\"21 - 25\"]\n",
    "age3 = [\"26 - 35\",\"36 - 45\"]\n",
    "age4 = [\"46 - 55\", \"56 - 65\"]\n",
    "age5 = [\"66 - 75\", \"75-100\"]\n",
    "#over 75 wouldnt work in the string so I did it separately\n",
    "for (row, col) in df.iterrows():\n",
    "\n",
    "    if str.lower(col.age_band_of_driver) in age1:\n",
    "        df['age_band_of_driver'].replace(to_replace=col.age_band_of_driver, \n",
    "                                         value='Under 16', inplace=True)\n",
    "\n",
    "    if str.lower(col.age_band_of_driver) in age2:\n",
    "        df['age_band_of_driver'].replace(to_replace=col.age_band_of_driver, \n",
    "                                         value='16-25', inplace=True)\n",
    "    \n",
    "    if str.lower(col.age_band_of_driver) in age3:\n",
    "        df['age_band_of_driver'].replace(to_replace=col.age_band_of_driver, \n",
    "                                         value='26-45', inplace=True)\n",
    "    if str.lower(col.age_band_of_driver) in age4:\n",
    "        df['age_band_of_driver'].replace(to_replace=col.age_band_of_driver, \n",
    "                                         value='46-65', inplace=True)\n",
    "    if str.lower(col.age_band_of_driver) in age5:\n",
    "        df['age_band_of_driver'].replace(to_replace=col.age_band_of_driver, \n",
    "                                         value='Over 65', inplace=True)\n",
    "#age_band_of_driver\n",
    "print(\"Distinct responses for age_band_of_driver:\\n\", set(df['age_band_of_driver']))\n",
    "# number_of_vehicles\n",
    "df['number_of_vehicles'].value_counts()/df.shape[0]*100\n",
    "#group number_of_vehicles\n",
    "\n",
    "def vehicles(num_veh):\n",
    "    if num_veh >=1 and num_veh <2:\n",
    "        return \"1\"\n",
    "    elif num_veh >=2 and num_veh <3:\n",
    "        return \"2\"\n",
    "    elif num_veh >=3 and num_veh <4:\n",
    "        return \"3\"\n",
    "    elif num_veh >= 4: \n",
    "        return \"4+\"\n",
    "  \n",
    "#apply function   \n",
    "df['number_of_vehicles']= df['number_of_vehicles'].apply(vehicles)\n",
    "# number_of_vehicles\n",
    "df['number_of_vehicles'].value_counts()/df.shape[0]*100\n",
    "df['number_of_vehicles'].dtypes\n",
    "df['number_of_vehicles']=df['number_of_vehicles'].astype('object')\n",
    "#creating seasons column for ML\n",
    "\n",
    "#creating season column\n",
    "\n",
    "def getSeason(month):\n",
    "    if (month == 12 or month == 1 or month == 2):\n",
    "       return \"winter\"\n",
    "    elif(month == 3 or month == 4 or month == 5):\n",
    "       return \"spring\"\n",
    "    elif(month == 6 or month== 7 or month == 8):\n",
    "       return \"summer\"\n",
    "    else:\n",
    "       return \"fall\"\n",
    "\n",
    "df['season'] = df['month'].apply(getSeason)\n",
    "# number_of_vehicles\n",
    "df['season'].value_counts()/df.shape[0]*100\n",
    "#go back to engine capacity CC and crete groups\n",
    "df.engine_capacity_cc.hist()\n",
    "def enginecap(eng_cc):\n",
    "    if eng_cc <=1500:\n",
    "        return \"small engine cc\"\n",
    "    if eng_cc >1500 and eng_cc <=2000:\n",
    "        return \"medium engine cc\"\n",
    "    if eng_cc >2000:\n",
    "        return \"large engine cc\"\n",
    "\n",
    "\n",
    "df['engine_capacity_cc_size'] = df['engine_capacity_cc'].apply(enginecap)\n",
    "df.engine_capacity_cc_size.value_counts()\n",
    "#Put above pickle in next full run\n",
    "#create new column for Machine Learning and Visualizations with Not Serious and Serious\n",
    "df['accident_seriousness'] = df['accident_severity']\n",
    "df['accident_seriousness'] = df['accident_seriousness'].replace(to_replace=\"Slight\", \n",
    "                                                                value=\"Not Serious\")\n",
    "df['accident_seriousness'] = df['accident_seriousness'].replace(to_replace=\"Serious\",\n",
    "                                                                value=\"Serious\")\n",
    "df['accident_seriousness'] = df['accident_seriousness'].replace(to_replace=\"Fatal\", \n",
    "                                                                value=\"Serious\")\n",
    "df.shape\n",
    "df.accident_seriousness.value_counts()\n",
    "#pickling everything to speed up restarting\n",
    "df.to_pickle(\"df.pkl\")\n",
    "#import pickled file\n",
    "df = pd.read_pickle(\"df.pkl\")\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
